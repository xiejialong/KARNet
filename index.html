<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/CTNet/img/favicon-32x32.png">
    <title>CTNet | </title>
    
<link rel="stylesheet" href="/CTNet/css/reset.css">

    
<link rel="stylesheet" href="/CTNet/css/style.css">

    
<link rel="stylesheet" href="/CTNet/css/markdown.css">

    
<link rel="stylesheet" href="/CTNet/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/CTNet/2023/08/21/paper/">Listen, Perceive, Grasp : CLIP-driven attribute-aware network for visual-language segmentation and grasping detection</a> -->
        <div class="post-title" >Listen, Perceive, Grasp : CLIP-driven attribute-aware network for visual-language segmentation and grasping detection</div>
        <div class="post-authors" >Jialong Xie, Jin Liu, Saike Huang, Fengyu Zhou</div>
        <div class="post-affiliations" >Shandong University</div>
        <div class="head-icon"> 

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://github.com/xiejialong/CTNet.git">
                    <i class="fa fa-github-square"></i>
                </a>
            

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://youtu.be/aVLP5_yC8l0">
                    <i class="fa fa-youtube-square"></i>
                </a>
            
            
        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >Visual-language grasping is a fundamental and challenging task in robotics manipulation and human-robot collaboration. However, previous approaches not only overlook fine-grained visual perception but also neglect the correlation between object attributes and candidate grasping detection. To this end, we propose a CLIP-driven attribute-aware network (CTNet) for robotic visual-language segmentation and grasping detection, enabling the robots to listen, perceive, and grasp in real-world applications. The listen stage leverages the pre-trained CLIP to understand and capture linguistic concepts. The perceive stage serves as mining object-orientated features and attributes (e.g. boundary and spatial location) as well as yielding the fine-grained segmentation mask. This stage can provide a prior constraint for grasping detection. The grasp stage aggregates the perceived attribute information to constrain and refine the spatial location and width of the grasping rectangle, facilitating the generation of a high-quality grasp pose. Lastly, we provide an extended large dataset refOCIDGrasp to train and test our method, resulting in a grasping accuracy of 97.76% and segmentation OIoU of 81.82%. The real-world robotic applications demonstrate the effectiveness and generalization of our proposed approach.</div>
        
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/aVLP5_yC8l0"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h3 id="The-listen-perceive-grasp-paradigm-for-robotic-grasp-reasoning"><a href="#The-listen-perceive-grasp-paradigm-for-robotic-grasp-reasoning" class="headerlink" title="The listen-perceive-grasp paradigm for robotic grasp reasoning"></a>The <em>listen-perceive-grasp</em> paradigm for robotic grasp reasoning</h3><p>The listen-perceive-grasp paradigm for robotic grasp reasoning. Given a natural language description of a desired object in the scene, a human is able to adeptly listen for the gist of the description and understand linguistic and visual concepts, then perceive and delimit the referred object, and ultimately grasp the target by factoring in the physical attribute such as shape, scale, and spatial location. This process is similarly applicable to robots.<br><img src="/CTNet/./images/Schematic.jpg" alt="listen-perceive-grasp paradigm"></p>
<h3 id="Overview-of-CTNet"><a href="#Overview-of-CTNet" class="headerlink" title="Overview of CTNet"></a>Overview of CTNet</h3><p>An overview of CTNet architecture. Given an image-query pair, we first employ listen module to understand and align visual and linguistic concepts. Then, the perceive module recursively extracts object-orientied features with visual attributes and generates the target mask corresponding to the description. Last,  The grasp module aggregates the attribute-based and semantic-rich features to refine the grasp pose of the desired object.<br><img src="/CTNet/./images/architecture.jpg" alt="listen-perceive-grasp paradigm"><br><img src="/CTNet/./images/TAMMI.jpg" alt="TAMMI"></p>
<h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p><img src="/CTNet/./images/visualization.jpg" alt="Visualization"></p>
<!-- ### Real-world robotic grasping experiments
<iframe src="//www.youtube.com/embed/IfjVsa1t2Jw" frameborder="0" allowfullscreen="" width='640' height='480'></iframe> -->
            <!-- <a class="read-more" href="/CTNet/2023/08/21/paper/"> ... </a> -->
        </div>
        <div class="post-date">2023.08.21</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright Â© 2023 CTNet</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/CTNet/css/a11y-dark.min.css">


<script src="/CTNet/js/highlight.min.js"></script>


<script src="/CTNet/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>