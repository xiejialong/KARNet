<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/xiejialong/KARNet/img/favicon.ico">
    <title>CTNet</title>
    
<link rel="stylesheet" href="/xiejialong/KARNet/css/reset.css">

    
<link rel="stylesheet" href="/xiejialong/KARNet/css/style.css">

    
<link rel="stylesheet" href="/xiejialong/KARNet/css/markdown.css">

    
<link rel="stylesheet" href="/xiejialong/KARNet/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/xiejialong/KARNet/2024/01/03/paper/">Listen, Perceive, Grasp: CLIP-driven attribute-aware network for language-conditioned visual segmentation and grasping</a> -->
        <div class="post-title" >Listen, Perceive, Grasp: CLIP-driven attribute-aware network for language-conditioned visual segmentation and grasping</div>
        <div class="post-authors" >Jialong Xie, Jin Liu, Saike Huang, Chaoqun Wang, and Fengyu Zhou</div>
        <div class="post-affiliations" >Shandong University</div>
        <div class="head-icon"> 

            
                <a class="icon-container" href="https://github.com/xiejialong/CTNet.git">
                    <i class="fa fa-github-square"></i>
                </a>
            

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://youtu.be/_7ieXE7Xms8">
                    <i class="fa fa-youtube-square"></i>
                </a>
            
            
        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >Endowing robots with the ability to understand natural language and execute grasping is a challenging task in a human-centric environment. Existing works on language-conditioned grasping achieve end-to-end grasping detection based on language. However, these works lack fine-grained visual grounding, resulting in cognitive deficits for robots. Moreover, they ignore the correlation between visual attributes of objects and grasping, leading to coarse grasp poses. To this end, we propose a CLIP-driven aTtribute-aware network (CTNet) for language-conditioned visual segmentation and grasping, enabling the robots to listen, perceive, and grasp the referred object in real-world applications. Specifically, we first employ Listen stage to understand basic linguistic and visual concepts. Subsequently, we introduce Perceive stage to mine multi-modal features and visual attribute cues (e.g., boundary and spatial location), then yield a language-conditioned segmentation mask. Further, we design Grasp stage to aggregate the perceived attribute information and refine the spatial location and grasping rectangle, generating a high-quality grasp pose. Lastly, we provide an extended large dataset Ref-OCID-Grasp to train and test our method, achieving a grasping accuracy of 97.76%  and segmentation OIoU of 91.82%. The real-world robotic applications demonstrate the effectiveness of our proposed approach.</div>
        <div class="post-video-title">
            <i class="fa fa-hand-o-right"></i><a  class= 'post-video-title' target="_blank" rel="noopener" href="https://youtu.be/_7ieXE7Xms8">Video on YouTube</a><i class="fa fa-hand-o-left"></i>
            
        </div>
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/_7ieXE7Xms8"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h2 id="The-listen-perceive-grasp-paradigm-for-robotic-grasp-reasoning"><a href="#The-listen-perceive-grasp-paradigm-for-robotic-grasp-reasoning" class="headerlink" title="The listen-perceive-grasp paradigm for robotic grasp reasoning"></a>The <em>listen-perceive-grasp</em> paradigm for robotic grasp reasoning</h2><p>The listen-perceive-grasp paradigm for robotic grasp reasoning. Given a natural language description of a desired object in the scene, a human is able to adeptly listen for the gist of the description and understand linguistic and visual concepts, then perceive and delimit the referred object, and ultimately grasp the target by factoring in the physical attribute such as shape, scale, and spatial location. This process is similarly applicable to robots. The visual attributes serve to constrain the both width and position of the grasping rectangle, thereby preventing collisions with other objects in the clutter.<br><img src="/xiejialong/KARNet/./images/Schematic.jpg" alt="listen-perceive-grasp paradigm"></p>
<h2 id="Overview-of-CTNet"><a href="#Overview-of-CTNet" class="headerlink" title="Overview of CTNet"></a>Overview of CTNet</h2><p>An overview of CTNet architecture. Given an image-query pair, we first employ listen module to understand and align visual and linguistic concepts. Then, the perceive module recursively extracts object-orientied features with visual attributes and generates the target mask corresponding to the description. Last,  The grasp module aggregates the attribute-based and semantic-rich features to refine the grasp pose of the desired object.<br><img src="/xiejialong/KARNet/./images/architecture.jpg" alt="listen-perceive-grasp paradigm"></p>
<h2 id="Dataset-examples"><a href="#Dataset-examples" class="headerlink" title="Dataset examples"></a>Dataset examples</h2><p>In order to train the proposed method to perceive and grasp the specific target corresponding to the language, we introduce a RefOCIDGrasp dataset based on <a target="_blank" rel="noopener" href="https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/">OCID</a>, <a href="https://github.com/stefan-ainetter/grasp_det_seg_cnn">OCID-grasp</a>, and <a href="https://github.com/lluma/OCID-Ref">OCID-Ref</a>  datasets.<br><img src="/xiejialong/KARNet/./images/examples.jpg" alt="Dataset Examples"></p>
<h2 id="Qualitative-results"><a href="#Qualitative-results" class="headerlink" title="Qualitative results"></a>Qualitative results</h2><p>Qualitative results of our proposed method for different objects under different or same scenes.<br><img src="/xiejialong/KARNet/./images/visualization.jpg" alt="Visualization"></p>

            <!-- <a class="read-more" href="/xiejialong/KARNet/2024/01/03/paper/"> ... </a> -->
        </div>
        <div class="post-date">2024.01.03</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright Â© 2023 CTNet</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/xiejialong/KARNet/css/a11y-dark.min.css">


<script src="/xiejialong/KARNet/js/highlight.min.js"></script>


<script src="/xiejialong/KARNet/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>